{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f02e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/drryodino246/csc542-gp-team164.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset('csv', data_files='/content/csc542-gp-team164/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_split = emotions['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = emotions_split['train']\n",
    "test_dataset = emotions_split['test']\n",
    "\n",
    "print(f\"Training data size: {len(train_dataset)}\")\n",
    "print(f\"Test data size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build-From-Scratch version ###\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ラベルエンコード\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_dataset['updated_emotion'])\n",
    "test_labels = label_encoder.transform(test_dataset['updated_emotion'])\n",
    "\n",
    "# トークナイズ（今回は簡易的に単語単位で分割）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Tokenizerだけ使う\n",
    "\n",
    "MAX_LEN = 64  # 短文ならこれで充分\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenize(texts)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_data = EmotionDataset(train_dataset['text'], train_labels)\n",
    "test_data = EmotionDataset(test_dataset['text'], test_labels)\n",
    "\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bf2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build-From-Scratch version ###\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, seq_len, embed_dim)\n",
    "        conv_x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # conv outputs\n",
    "        pooled_x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_x]\n",
    "        out = torch.cat(pooled_x, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "    \n",
    "###################################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e13b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_path = \"/content/glove.6B.100d.txt\"     # This could be a hyper-parameter.\n",
    "glove_embeddings = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "# Colab上でデータから語彙を作る\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield text.lower().split()\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset['text']), specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# GloVe埋め込みをvocabに合わせて並べる\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
    "\n",
    "for i, token in enumerate(vocab.get_itos()):\n",
    "    vector = glove_embeddings.get(token)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes, kernel_sizes=[3,4,5], num_filters=100):\n",
    "        super(TextCNN, self).__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # ← ここ大事！ # This could be a hyper-parameter (freeze = True or False)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        x = x.unsqueeze(1)     # (batch, 1, seq_len, embed_dim)\n",
    "        conv_x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        pooled_x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_x]\n",
    "        out = torch.cat(pooled_x, dim=1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62308d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab, max_length=100):\n",
    "        self.dataset = dataset\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text'].lower().split()\n",
    "        tokens = [self.vocab[token] for token in text]\n",
    "\n",
    "        # 長さを固定（パディング or 切り捨て）\n",
    "        if len(tokens) < self.max_length:\n",
    "            tokens += [self.vocab[\"<pad>\"]] * (self.max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_length]\n",
    "\n",
    "        label = self.dataset[idx]['updated_emotion']\n",
    "        return torch.tensor(tokens), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fcbb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TextDataset(train_dataset, vocab)\n",
    "test_data = TextDataset(test_dataset, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(set(train_dataset['updated_emotion']))  # クラス数推定\n",
    "\n",
    "model = TextCNN(embedding_matrix, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13367c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
